<VirtualHost *:80>
  ServerName localhost
  DocumentRoot /elixir/application/frontend

  Alias /robots.txt /elixir/application/robots/robots-deny-all.txt
  <Location "/robots.txt">
    SetHandler None
    Require all granted
  </Location>
  
  WSGIDaemonProcess el threads=5
  WSGIScriptAlias /api /elixir/application/backend/elixirapp/wsgi.py
  WSGIPassAuthorization On

  # Set env if User-Agent looks like a bot
  SetEnvIfNoCase User-Agent "(googlebot|bingbot|duckduckbot|twitterbot|facebookexternalhit|linkedinbot|slackbot|discordbot|yandex|baiduspider|embedly|quora|pinterest|whatsapp|telegram|applebot)" is_bot
  
  # Exclude API and static asset paths from bot handling
  RewriteEngine on
  RewriteCond %{ENV:is_bot} =1
  RewriteCond %{REQUEST_URI} !^/api/ [NC]
  RewriteCond %{REQUEST_URI} !^/(dist|static|img|css|js|fonts)/ [NC]
  RewriteCond %{REQUEST_URI} !^/(favicon\.ico|robots\.txt|sitemap\.xml) [NC]
  # Make responses vary by UA so caches don't mix bot/human content
  Header set Vary "User-Agent" env=is_bot
  # Proxy the request to Rendertron (preserves scheme and host)
  RewriteRule ^(.*)$ http://rendertron:3000/render/%{REQUEST_SCHEME}://%{HTTP_HOST}%{REQUEST_URI} [P,L]

  # --- Instruct crawlers not to index search pages (/t) ---
  <LocationMatch "^/t(/|$)">
    Header set X-Robots-Tag "noindex, follow"
  </LocationMatch>

  <Directory /elixir/application/frontend>
    # Don't rewrite files or directories
    RewriteCond %{REQUEST_FILENAME} -f [OR]
    RewriteCond %{REQUEST_FILENAME} -d
    RewriteRule ^ - [L]

    # Rewrite everything else to index.html to allow html5 state links
    RewriteRule ^ index.html [L]
  </Directory>
  <LocationMatch />
    Options +Indexes +FollowSymLinks +MultiViews
    AllowOverride all
    Require all granted
    WSGIScriptReloading On
    WSGIApplicationGroup %{GLOBAL}
  </LocationMatch>

  # CORS support
  <IfModule mod_headers.c>
    Header set Access-Control-Allow-Origin "*"
  </IfModule>
</VirtualHost>